{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ugYIZG_vwU"
      },
      "source": [
        "# Read dataset and create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sUdX_1lQ7Zf8"
      },
      "outputs": [],
      "source": [
        "# Import torch and CIFAR dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import matplotlib and numpy for graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dy0dXGIAAs-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 64\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Import CIFAR dataset, define labbels and load training and validation dataset\n",
        "Reference for loading dataset: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "Reference for augmentation: https://pytorch.org/vision/stable/transforms.html\n",
        "'''\n",
        "batch_size=64 \n",
        "print('Batch size:', batch_size)\n",
        "\n",
        "# Normalisation and std values for RGB in dataset\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Data augmentation for training set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # Randomly crop the image with padding\n",
        "    transforms.RandomHorizontalFlip(),    # Randomly flip the image horizontally\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust brightness, contrast, etc.\n",
        "    transforms.RandomRotation(15),        # Randomly rotate the image by up to 15 degrees\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Randomly translate the image\n",
        "    transforms.ToTensor(),                # Convert image to tensor\n",
        "    transforms.Normalize(mean=mean, std=std),  # Normalize with mean and std\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33))  # Randomly erase a portion of the image (optional)\n",
        "])\n",
        "\n",
        "# No augmentation for test set (only normalization)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)  # Normalize with mean and std\n",
        "])\n",
        "\n",
        "# Load training and testing datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define labels\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'lorry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ejYbu1lREwwA"
      },
      "outputs": [],
      "source": [
        "# # From the PyTorch's tutorial on image classification\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# def imshow(img):\n",
        "#     '''\n",
        "#     Show an image\n",
        "#     Input: image file to show\n",
        "#     Output: image\n",
        "#     '''\n",
        "#     img = img / 2 + 0.5     # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# # Get random training images\n",
        "# dataiter = iter(trainloader)\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# # Show images\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# # Print labels\n",
        "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28nn7m9c_1AK"
      },
      "source": [
        "# Main model\n",
        "Divided as such:\n",
        "\n",
        "\n",
        "*   **Stem**: takes the images as inputs, extracts features from them\n",
        "*   **Backbone**: made up of *K* branches, made up of an expert branch\n",
        "*   **Classifier**: takes input from the last block\n",
        "*   **Model**: wraps all together\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjsIHiWpC7ir"
      },
      "source": [
        "## Stem\n",
        "*   Takes images as inputs\n",
        "*   Extracts a feature representation from them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GPmPcMg3MKuo"
      },
      "outputs": [],
      "source": [
        "class Stem(nn.Module):\n",
        "  '''\n",
        "  Extract features using a Resnet-18 stem\n",
        "  Reference: Week 09 Lab\n",
        "  '''\n",
        "  def __init__(self, input_channels, middle_channels, output_channels):\n",
        "     super(Stem,self).__init__()\n",
        "     # Default parameters\n",
        "     kernel_size=3\n",
        "     stride=1\n",
        "     padding=1\n",
        "     \n",
        "     # Combine multiple layers\n",
        "     self.stem = nn.Sequential(\n",
        "       nn.Conv2d(input_channels, middle_channels, kernel_size = kernel_size, stride = stride, padding = padding),\n",
        "       nn.BatchNorm2d(middle_channels), \n",
        "       nn.ReLU(inplace=True),\n",
        "       nn.Conv2d(middle_channels, middle_channels,kernel_size = kernel_size, stride = stride, padding = padding),\n",
        "       nn.BatchNorm2d(middle_channels),\n",
        "       nn.ReLU(inplace=True),\n",
        "       nn.MaxPool2d(2), # Half the size of the image\n",
        "       nn.Conv2d(middle_channels, output_channels, kernel_size = kernel_size, stride = stride, padding = padding),\n",
        "       nn.BatchNorm2d(output_channels),\n",
        "       nn.ReLU(inplace=True),\n",
        "       nn.MaxPool2d(2) # Half the size of the image\n",
        "       )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.stem(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teldaHE3NAuT"
      },
      "source": [
        "## Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q50v5kSs9Lbc"
      },
      "outputs": [],
      "source": [
        "class ExpertBranch(nn.Module):\n",
        "  '''\n",
        "  Expert branch predicting vector a with K elements from input tensor X\n",
        "  '''\n",
        "  def __init__(self, input_channels, k, r):\n",
        "    super(ExpertBranch,self).__init__()\n",
        "    # Spatially pool x\n",
        "    self.pool= nn.AdaptiveAvgPool2d(1)\n",
        "    #Forward through fc1, reducing by r\n",
        "    self.fc1= nn.Linear(input_channels, input_channels//r)\n",
        "    # Activation function ReLu\n",
        "    self.relu= nn.ReLU()\n",
        "    # Forward through fc2\n",
        "    self.fc2= nn.Linear(input_channels//r,k)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Spatially pool X\n",
        "    x = self.pool(x)\n",
        "    # Forward through fc1, reducing by r\n",
        "    x= x.squeeze(-1).squeeze(-1)\n",
        "    x = self.fc1(x)\n",
        "    # Processed through non-linear activation g\n",
        "    x = F.relu(x)\n",
        "    # Pass through fc2\n",
        "    x = self.fc2(x)\n",
        "    # Forward with softmax\n",
        "    x = F.softmax(x,dim=1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xUNibZ2FNE3V"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  '''\n",
        "  Block\n",
        "  '''\n",
        "  def __init__(self, input_channels, output_channels, k, r):\n",
        "    super(Block, self).__init__()\n",
        "    # Default parameters\n",
        "    kernel_size=3\n",
        "    stride=1\n",
        "    padding=1\n",
        "    # Set parameters\n",
        "    self.k= k\n",
        "    self.expertBranch = ExpertBranch(input_channels, k=k, r=r)\n",
        "    # Input from first block\n",
        "    # Input from previous block for rest\n",
        "    # Generate vector a with K elements from X as a= E(X)\n",
        "    # Create K convolutional layers\n",
        "    self.convs= nn.ModuleList([\n",
        "        nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride= stride, padding=padding)\n",
        "        for _ in range(k)\n",
        "    ])\n",
        "\n",
        "  def forward(self,x):\n",
        "    identity= x\n",
        "    # Vector a from expert branch\n",
        "    a = self.expertBranch(x)\n",
        "    # Convolutional layers \n",
        "    conv_outputs = [conv(x) for conv in self.convs]\n",
        "    stacked = torch.stack(conv_outputs, dim=1)\n",
        "    # Create vector O\n",
        "    a= a.view(a.size(0), self.k, 1,1,1)\n",
        "\n",
        "    out = (a* stacked).sum(dim=1)\n",
        "    # Skip connection to stablise gradient descent\n",
        "    out += identity\n",
        "    # out = F.relu(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NatobdwZM-dN"
      },
      "source": [
        "## Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2uZ7am7LOxUi"
      },
      "outputs": [],
      "source": [
        "class Backbone(nn.Module):\n",
        "  '''\n",
        "  N blocks\n",
        "  '''\n",
        "  def __init__(self, input_channels, hidden_channels, num_blocks, k, r):\n",
        "    super(Backbone, self).__init__()\n",
        "    self.blocks= nn.ModuleList()\n",
        "\n",
        "    # First block takes input from stem\n",
        "    self.blocks.append(Block(input_channels, hidden_channels, k=k, r=r))\n",
        "\n",
        "    # Rest of blocks take input form previous block\n",
        "    for _ in range(1, num_blocks):\n",
        "      self.blocks.append(Block(hidden_channels, hidden_channels, k=k, r=r))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for idx, block in enumerate(self.blocks):\n",
        "      x = block(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgi2QD7KNC9v"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t8QmbjyB4feJ"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_channels, num_classes, use_mlp):\n",
        "    super(Classifier,self).__init__()\n",
        "    # Default parameters\n",
        "    dropout_rate=0.25\n",
        "    # Spatially pool\n",
        "    self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.use_mlp= use_mlp\n",
        "\n",
        "    if use_mlp:\n",
        "      self.classifier= nn.Sequential(\n",
        "          nn.Linear(input_channels, input_channels*2),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(dropout_rate), # Deeper network with 3 layers\n",
        "          nn.Linear(input_channels*2, input_channels),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(dropout_rate),\n",
        "          nn.Linear(input_channels, num_classes)\n",
        "      )\n",
        "    else:\n",
        "      self.classifier= nn.Linear(input_channels, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(x).squeeze(-1).squeeze(-1)\n",
        "    out = self.classifier(x)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySRxujey4gN3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YY9DBSBo4jpP"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, input_channels, output_channels, middle_channels, hidden_channels, num_blocks, k, r, num_classes, use_mlp):\n",
        "    super(Model, self).__init__()\n",
        "    # Call stem\n",
        "    self.stem= Stem(\n",
        "      input_channels=input_channels,\n",
        "      middle_channels=middle_channels,\n",
        "      output_channels=output_channels\n",
        "    )\n",
        "    # Call backbone\n",
        "    self.backbone= Backbone(\n",
        "      input_channels=output_channels, \n",
        "      hidden_channels= hidden_channels, \n",
        "      num_blocks=num_blocks,\n",
        "      k=k, \n",
        "      r=r)\n",
        "    # Call classifier\n",
        "    self.classifier= Classifier(\n",
        "      input_channels=hidden_channels, \n",
        "      num_classes=num_classes,\n",
        "      use_mlp= use_mlp)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x= self.stem(x)\n",
        "    x= self.backbone(x)\n",
        "    x= self.classifier(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC2qAy2b_3KO"
      },
      "source": [
        "# Create the loss and optmiser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGJbtSC3EcHy"
      },
      "outputs": [],
      "source": [
        "model = Model(\n",
        "    input_channels=3,\n",
        "    output_channels=128,\n",
        "    middle_channels=64,\n",
        "    hidden_channels=128,\n",
        "    num_blocks=7,\n",
        "    k=2,\n",
        "    r=8,\n",
        "    num_classes=10,\n",
        "    use_mlp=True\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.0001, weight_decay=1e-4, momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlNFWcuJ_8dY"
      },
      "source": [
        "# Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdK9bHyHRaVt",
        "outputId": "f59cac49-555d-4148-9435-d29fb3076c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/782 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1702400440653/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.1710 | Accuracy: 23.27%\n",
            "Val   Loss: 2.3050 | Accuracy: 10.00%\n",
            "Saved best model.\n",
            "\n",
            "Epoch 2/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 564.5705 | Accuracy: 9.83%\n",
            "Val   Loss: 2.3026 | Accuracy: 10.00%\n",
            "\n",
            "Epoch 3/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                          \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, testloader, criterion, device)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Log metrics\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader), \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Set up device \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Save model\n",
        "model.to(device)\n",
        "\n",
        "# Log training \n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training and Validation Loops \n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), 100 * correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return loss / len(loader), 100 * correct / total\n",
        "\n",
        "# Main Loop \n",
        "epochs = 25\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    train_loss, train_acc = train(model, trainloader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, testloader, criterion, device)\n",
        "\n",
        "\n",
        "    # Log metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Saved best model.\")\n",
        "\n",
        "print(\"\\nTraining Complete\")\n",
        "\n",
        "# Print Final Averages \n",
        "avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "avg_train_acc = sum(train_accuracies) / len(train_accuracies)\n",
        "avg_val_acc = sum(val_accuracies) / len(val_accuracies)\n",
        "\n",
        "print(\"\\nFinal Averages Over All Epochs\")\n",
        "print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
        "print(f\"Average Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "print(f\"Average Val   Loss: {avg_val_loss:.4f}\")\n",
        "print(f\"Average Val   Accuracy: {avg_val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# Plot results\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"loss_curve.png\")\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure()\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title(\"Accuracy Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"accuracy_curve.png\")\n",
        "\n",
        "print(\"Plots saved: loss_curve.png and accuracy_curve.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ### Data loading and augmentation from test_train.py ###\n",
        "# # Added Normalize with the standard CIFAR-10 statistics\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.RandomCrop(32, padding=4),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
        "# ])\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
        "# ])\n",
        "\n",
        "# # Downloading and creating the Datasets here\n",
        "# train_dataset = torchvision.datasets.CIFAR10(\n",
        "#     root='./data', train=True, download=True, transform=transform_train\n",
        "# )\n",
        "# test_dataset = torchvision.datasets.CIFAR10(\n",
        "#     root='./data', train=False, download=True, transform=transform_test\n",
        "# )\n",
        "\n",
        "# # Creating DataLoaders here\n",
        "# batch_size = 128\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "# ### Training utilities from test_train.py (with fixed method names) ###\n",
        "# class Accumulator:\n",
        "#     \"\"\"For accumulating sums over n variables.\"\"\"\n",
        "#     def __init__(self, n):\n",
        "#         self.data = [0.0] * n\n",
        "#     def add(self, *args):\n",
        "#         self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "#     def reset(self):\n",
        "#         self.data = [0.0] * len(self.data)\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.data[idx]\n",
        "\n",
        "# def accuracy(y_hat, y):\n",
        "#     \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "#     if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "#         y_hat = y_hat.argmax(axis=1)\n",
        "#     cmp = (y_hat.type(y.dtype) == y)\n",
        "#     return float(torch.sum(cmp))\n",
        "\n",
        "# def evaluate_accuracy(net, data_iter, device): \n",
        "#     \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "#     net.eval()\n",
        "#     metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "#     with torch.no_grad():\n",
        "#         for X, y in data_iter:\n",
        "#             X, y = X.to(device), y.to(device)\n",
        "#             metric.add(accuracy(net(X), y), y.numel())\n",
        "#     return metric[0] / metric[1]\n",
        "\n",
        "# def train_epoch(net, train_iter, loss, optimizer, device):\n",
        "#     \"\"\"Training function for one epoch.\"\"\"\n",
        "#     net.train()\n",
        "#     metric = Accumulator(3)  # train_loss, train_acc, num_examples\n",
        "#     for X, y in train_iter:\n",
        "#         X, y = X.to(device), y.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         y_hat = net(X)\n",
        "#         l = loss(y_hat, y)\n",
        "#         l.backward()\n",
        "#         optimizer.step()\n",
        "#         metric.add(float(l) * len(y), accuracy(y_hat, y), y.numel())\n",
        "#     return metric[0] / metric[2], metric[1] / metric[2]\n",
        "\n",
        "# def train_model(net, train_iter, test_iter, loss, optimizer, num_epochs, device):\n",
        "#     \"\"\"Train and evaluate a model.\"\"\"\n",
        "#     print('-' * 50)\n",
        "#     print('Starting training...')\n",
        "    \n",
        "#     train_losses = []\n",
        "#     train_accs = []\n",
        "#     test_accs = []\n",
        "    \n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_metrics = train_epoch(net, train_iter, loss, optimizer, device)\n",
        "#         test_acc = evaluate_accuracy(net, test_iter, device)\n",
        "#         train_loss, train_acc = train_metrics\n",
        "        \n",
        "#         train_losses.append(train_loss)\n",
        "#         train_accs.append(train_acc)\n",
        "#         test_accs.append(test_acc)\n",
        "        \n",
        "#         print(f'Epoch {epoch + 1}:')\n",
        "#         print(f'  Train loss: {train_loss:.3f}')\n",
        "#         print(f'  Train accuracy: {train_acc:.3f} ({train_acc*100:.1f}%)')\n",
        "#         print(f'  Test accuracy:  {test_acc:.3f} ({test_acc*100:.1f}%)')\n",
        "    \n",
        "#     # Plot metrics\n",
        "#     plt.figure(figsize=(12, 4))\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(train_losses, label='train loss')\n",
        "#     plt.xlabel('epoch')\n",
        "#     plt.ylabel('loss')\n",
        "#     plt.legend()\n",
        "    \n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot([x*100 for x in train_accs], label='train acc (%)')\n",
        "#     plt.plot([x*100 for x in test_accs], label='test acc (%)')\n",
        "#     plt.xlabel('epoch')\n",
        "#     plt.ylabel('accuracy (%)')\n",
        "#     plt.legend()\n",
        "#     plt.savefig('training_results.png')\n",
        "#     plt.show()\n",
        "    \n",
        "#     return train_losses, train_accs, test_accs\n",
        "\n",
        "# ### Main execution block ###\n",
        "# if __name__ == '__main__':\n",
        "#     # Device configuration\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     if device.type == 'cuda':\n",
        "#         print('GPU training enabled')  # Simplified device info\n",
        "    \n",
        "#     # Create your model from mymodel.py\n",
        "#     model = Model(\n",
        "#         stem_channels=128,\n",
        "#         hidden_channels=128,\n",
        "#         num_blocks=3,\n",
        "#         k=4,\n",
        "#         r=4,\n",
        "#         num_classes=10,\n",
        "#         use_mlp=True\n",
        "#     ).to(device)\n",
        "    \n",
        "#     # Define loss function and optimizer\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "#     # Train the model\n",
        "#     train_losses, train_accs, test_accs = train_model(\n",
        "#         model, train_loader, test_loader, criterion, optimizer, num_epochs=25, device=device\n",
        "#     )\n",
        "    \n",
        "#     # Save model\n",
        "#     torch.save(model.state_dict(), \"best_model.pth\")\n",
        "#     print(\"Model saved as best_model.pth\")\n",
        "    \n",
        "#     # Print final metrics\n",
        "#     print(\"\\nFinal Metrics:\")\n",
        "#     print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
        "#     print(f\"Final train accuracy: {train_accs[-1]*100:.2f}%\")\n",
        "#     print(f\"Final test accuracy: {test_accs[-1]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnF3CN1zohxI"
      },
      "source": [
        "Averages:\n",
        "\n",
        "\n",
        "*   Train Loss: 1.7223, Accuracy: 38.21%, Validation Loss: 1.7194, Accuracy: 38.25%\n",
        "*   Train Loss: 1.7106, Accuracy: 34.80%, Validation Loss: 1.7984, Accuracy: 35.98%\n",
        "\n",
        "*   Train Loss: 1.8150, Accuracy: 34.54%, Val   Loss: 1.7848 Accuracy: 36.13%\n",
        "\n",
        "*   Train Loss: 1.9579, Accuracy: 28.84%, Val   Loss: 1.8691, Accuracy: 32.51%\n",
        "*   Train Loss: 1.9712, Accuracy: 27.54%, Val   Loss: 1.9107 ,Accuracy: 30.24%\n",
        "*   Train Loss: 2.1609, Accuracy: 16.97%, Val   Loss: 2.1343, Accuracy: 18.28%\n",
        "*   Train Loss: 1.9798, Accuracy: 27.24%, Val   Loss: 1.9312, Accuracy: 29.65%\n",
        "*   Train Loss: 1.4970, Accuracy: 44.11%, Val   Loss: 1.3675, Accuracy: 48.65%\n",
        "*   Train Loss: 1.3648, Accuracy: 51.66%, Val   Loss: 1.2319, Val   Accuracy: 55.56%\n",
        "*   Train Loss: 0.7390, Accuracy: 74.50%, Val   Loss: 0.8193, Val   Accuracy: 72.47%\n",
        "*  Train Loss: 0.7262, Accuracy: 74.99%, Val   Loss: 0.8539, Accuracy: 71.57%\n",
        "*   Train Loss: 0.6575, Accuracy: 76.88%, Val   Loss: 0.7876, Accuracy: 73.31%\n",
        "*   Train Loss: 0.6564, Accuracy: 76.91%, Val   Loss: 0.7731, Accuracy: 73.89%\n",
        "*   Train Loss: 0.6747, Accuracy: 76.24%, Val   Loss: 0.7645, Accuracy: 73.83%\n",
        "*   Train Loss: 0.7119, Accuracy: 74.75%, Val   Loss: 0.8092, Accuracy: 72.01%\n",
        "* Train Loss: 1.0820, Accuracy: 61.24%,  Val   Loss: 0.9241, Accuracy: 66.86%\n",
        "* Train Accuracy: 62.14%, Val   Loss: 0.8826, Accuracy: 68.16%\n",
        "* Train Loss: 1.1482, Accuracy: 59.56%, Val   Loss: 0.9446, Accuracy: 66.12%\n",
        "* Train Loss: 1.0876, Accuracy: 61.30%, Val   Loss: 0.9327,Accuracy: 66.59%\n",
        "* Train Loss: 1.1938, Accuracy: 57.16%, Val   Loss: 1.0265, Accuracy: 62.66%\n",
        "* Train Loss: 1.3364, Accuracy: 51.05%, Val   Loss: 1.1814, Accuracy: 56.93%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
