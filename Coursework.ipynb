{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ugYIZG_vwU"
      },
      "source": [
        "# Read dataset and create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sUdX_1lQ7Zf8"
      },
      "outputs": [],
      "source": [
        "# Import torch and CIFAR dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Import matplotlib and numpy for graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Dy0dXGIAAs-N"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Import CIFAR dataset, define labbels and load training and validation dataset\n",
        "Reference: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "'''\n",
        "# Initialise vectors and batch size\n",
        "transform= transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "batch_size=16\n",
        "# Load training and testing dataset\n",
        "trainset= torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform= transform)\n",
        "trainloader= torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testset= torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader= torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "# Define labels\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'lorry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ejYbu1lREwwA"
      },
      "outputs": [],
      "source": [
        "# # From the PyTorch's tutorial on image classification\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# def imshow(img):\n",
        "#     '''\n",
        "#     Show an image\n",
        "#     Input: image file to show\n",
        "#     Output: image\n",
        "#     '''\n",
        "#     img = img / 2 + 0.5     # unnormalize\n",
        "#     npimg = img.numpy()\n",
        "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "#     plt.show()\n",
        "\n",
        "# # Get random training images\n",
        "# dataiter = iter(trainloader)\n",
        "# images, labels = next(dataiter)\n",
        "\n",
        "# # Show images\n",
        "# imshow(torchvision.utils.make_grid(images))\n",
        "# # Print labels\n",
        "# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28nn7m9c_1AK"
      },
      "source": [
        "# Main model\n",
        "Divided as such:\n",
        "\n",
        "\n",
        "*   **Stem**: takes the images as inputs, extracts features from them\n",
        "*   **Backbone**: made up of *K* branches, made up of an expert branch\n",
        "*   **Classifier**: takes input from the last block\n",
        "*   **Model**: wraps all together\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjsIHiWpC7ir"
      },
      "source": [
        "## Stem\n",
        "*   Takes images as inputs\n",
        "*   Extracts a feature representation from them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JPw9CKMrDGzl"
      },
      "outputs": [],
      "source": [
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super().__init__()\n",
        "#     self.conv1= nn.Conv2d(3,6,5)\n",
        "#     self.pool= nn.MaxPool2d(2,2)\n",
        "#     self.conv2= nn.Conv2d(6,16,5)\n",
        "#     self.fc1= nn.Linear(16 * 5 * 5, 120)\n",
        "#     self.fc2=nn.Linear(120,84)\n",
        "#     self.fc3=nn.Linear(84,10)\n",
        "\n",
        "#   def forward(self,x):\n",
        "#     x= self.pool(F.relu(self.conv1(x)))\n",
        "#     x= self.pool(F.relu(self.conv2(x)))\n",
        "#     x= torch.flatten(x,1)\n",
        "#     x= F.relu(self.fc1(x))\n",
        "#     x= F.relu(self.fc2(x))\n",
        "#     x= self.fc3(x)\n",
        "#     return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GPmPcMg3MKuo"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Stem(nn.Module):\n",
        "  '''\n",
        "  Extract features using a simple convolutional layer\n",
        "  '''\n",
        "  def __init__(self) -> None:\n",
        "     super(Stem,self).__init__()\n",
        "     self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    # print(\"Stem output shape in forward pass:\", x.shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teldaHE3NAuT"
      },
      "source": [
        "## Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "q50v5kSs9Lbc"
      },
      "outputs": [],
      "source": [
        "class ExpertBranch(nn.Module):\n",
        "  '''\n",
        "  Expert branch predicting vector a with K elements from input tensor X\n",
        "  '''\n",
        "  def __init__(self, input_channels, k=4, r=4):\n",
        "    super(ExpertBranch,self).__init__()\n",
        "    # Spatially pool x\n",
        "    self.pool= nn.AdaptiveAvgPool2d(1)\n",
        "    #Forward through fc1, reducing by r\n",
        "    self.fc1= nn.Linear(input_channels, input_channels//r)\n",
        "    # Activation function ReLu\n",
        "    self.relu= nn.ReLU()\n",
        "    # Forward through fc2\n",
        "    self.fc2= nn.Linear(input_channels//r,k)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Spatially pool X\n",
        "    x = self.pool(x)\n",
        "    # Forward through fc1, reducing by r\n",
        "    x= x.squeeze(-1).squeeze(-1)\n",
        "    x = self.fc1(x)\n",
        "    # Processed through non-linear activation g\n",
        "    x = F.relu(x)\n",
        "    # Pass through fc2\n",
        "    x = self.fc2(x)\n",
        "    # Forward with softmax\n",
        "    x = F.softmax(x,dim=1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUNibZ2FNE3V"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  '''\n",
        "  Block\n",
        "  '''\n",
        "  def __init__(self, input_channels, output_channels, k=4, r=4, kernel_size=3, stride=1, padding=1):\n",
        "    super(Block, self).__init__()\n",
        "    self.k= k\n",
        "    self.expertBranch = ExpertBranch(input_channels, k=k, r=r)\n",
        "    # Input from first block\n",
        "    # Input from previous block for rest\n",
        "    # Generate vector a with K elements from X as a= E(X)\n",
        "    # Create K convolutional layers\n",
        "    self.convs= nn.ModuleList([\n",
        "        nn.Conv2d(input_channels, output_channels, kernel_size=kernel_size, stride= stride, padding=padding)\n",
        "        for _ in range(k)\n",
        "    ])\n",
        "\n",
        "  def forward(self,x):\n",
        "    a = self.expertBranch(x)\n",
        "    conv_outputs = [conv(x) for conv in self.convs]\n",
        "    stacked = torch.stack(conv_outputs, dim=1)\n",
        "    a= a.view(a.size(0), self.k, 1,1,1)\n",
        "\n",
        "    out = (a* stacked).sum(dim=1)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NatobdwZM-dN"
      },
      "source": [
        "## Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2uZ7am7LOxUi"
      },
      "outputs": [],
      "source": [
        "class Backbone(nn.Module):\n",
        "  '''\n",
        "  N blocks\n",
        "  '''\n",
        "  def __init__(self, input_channels=64, hidden_channels=64, num_blocks=3, k=4, r=4):\n",
        "    super(Backbone, self).__init__()\n",
        "    self.blocks= nn.ModuleList()\n",
        "\n",
        "    # First block takes input from stem\n",
        "    self.blocks.append(Block(input_channels, hidden_channels, k=k, r=r))\n",
        "\n",
        "    # Rest of blocks take input form previous block\n",
        "    for _ in range(1, num_blocks):\n",
        "      self.blocks.append(Block(hidden_channels, hidden_channels, k=k, r=r))\n",
        "\n",
        "  def forward(self, x):\n",
        "    for idx, block in enumerate(self.blocks):\n",
        "      # print(f\"Backbone Block {idx+1} output shape:\", x.shape)\n",
        "      x = block(x)\n",
        "      # print(f\"Backbone Block {idx+1} output shape:\", x.shape)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgi2QD7KNC9v"
      },
      "source": [
        "## Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "t8QmbjyB4feJ"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self, input_channels, num_classes=10, use_mlp=False):\n",
        "    super(Classifier,self).__init__()\n",
        "    # Spatially pool\n",
        "    self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.use_mlp= use_mlp\n",
        "\n",
        "    if use_mlp:\n",
        "      self.classifier= nn.Sequential(\n",
        "          nn.Linear(input_channels, input_channels//2),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(0.5), # Deeper network with 3 layers\n",
        "          nn.Linear(input_channels//2, input_channels//2),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(input_channels //2, num_classes)\n",
        "      )\n",
        "    else:\n",
        "      self.classifier= nn.Linear(input_channels, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(x).squeeze(-1).squeeze(-1)\n",
        "    # print(\"Classifier input shape:\", x.shape)\n",
        "    out = self.classifier(x)\n",
        "    # print(\"Classifier output shape (logits):\", out.shape)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySRxujey4gN3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "YY9DBSBo4jpP"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, stem_channels= 64, hidden_channels=64, num_blocks=3, k=4, r=4, num_classes=10, use_mlp=False):\n",
        "    super(Model, self).__init__()\n",
        "    # Call stem\n",
        "    self.stem= Stem()\n",
        "    # Call backbone\n",
        "    self.backbone= Backbone(input_channels=stem_channels, hidden_channels= hidden_channels, num_blocks=num_blocks, k=k, r=r)\n",
        "    # Call classifier\n",
        "    self.classifier= Classifier(input_channels=hidden_channels, num_classes=num_classes, use_mlp= use_mlp)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x= self.stem(x)\n",
        "    x= self.backbone(x)\n",
        "    x= self.classifier(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC2qAy2b_3KO"
      },
      "source": [
        "# Create the loss and optmiser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "DGJbtSC3EcHy"
      },
      "outputs": [],
      "source": [
        "model = Model(\n",
        "    stem_channels=64,\n",
        "    hidden_channels=64,\n",
        "    num_blocks=3,\n",
        "    k=4,\n",
        "    r=4,\n",
        "    num_classes=10,\n",
        "    use_mlp=True\n",
        ")\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlNFWcuJ_8dY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdK9bHyHRaVt",
        "outputId": "f59cac49-555d-4148-9435-d29fb3076c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/3125 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  35%|███▌      | 1099/3125 [00:11<00:24, 83.37it/s] "
          ]
        }
      ],
      "source": [
        "# ---------- 1. Set up device ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------- 2. Load CIFAR-10 ----------\n",
        "# # Initialise vectors and batch size\n",
        "# transform= transforms.Compose([transforms.ToTensor(),\n",
        "#                                transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
        "# batch_size=4\n",
        "# # Load training and testing dataset\n",
        "# trainset= torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform= transform)\n",
        "# trainloader= torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "# testset= torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "# testloader= torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "# # Define labels\n",
        "# classes = ('plane', 'car', 'bird', 'cat',\n",
        "#            'deer', 'dog', 'frog', 'horse', 'ship', 'lorry')\n",
        "\n",
        "# ---------- 3. Initialise Model ----------\n",
        "# model = Model(\n",
        "#     stem_channels=64,\n",
        "#     hidden_channels=64,\n",
        "#     num_blocks=3,\n",
        "#     k=4,\n",
        "#     r=4,\n",
        "#     num_classes=10,\n",
        "#     use_mlp=True\n",
        "# ).to(device)\n",
        "\n",
        "# Save model\n",
        "model.to(device)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ---------- Log training ----------\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# ---------- Training + Validation Loops ----------\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return running_loss / len(loader), 100 * correct / total\n",
        "\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss += criterion(outputs, labels).item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return loss / len(loader), 100 * correct / total\n",
        "\n",
        "# ---------- 5. Main Loop ----------\n",
        "epochs = 10\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    train_loss, train_acc = train(model, trainloader, criterion, optimizer, device)\n",
        "    val_loss, val_acc = evaluate(model, testloader, criterion, device)\n",
        "\n",
        "\n",
        "    # Log metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")\n",
        "    print(f\"Val   Loss: {val_loss:.4f} | Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(\"Saved best model.\")\n",
        "\n",
        "print(\"\\nTraining Complete\")\n",
        "\n",
        "# ---------- 8. Print Final Averages ----------\n",
        "avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "avg_train_acc = sum(train_accuracies) / len(train_accuracies)\n",
        "avg_val_acc = sum(val_accuracies) / len(val_accuracies)\n",
        "\n",
        "print(\"\\nFinal Averages Over All Epochs\")\n",
        "print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
        "print(f\"Average Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "print(f\"Average Val   Loss: {avg_val_loss:.4f}\")\n",
        "print(f\"Average Val   Accuracy: {avg_val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "# ---------- 7. Plot Results ----------\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure()\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title(\"Loss Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"loss_curve.png\")\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure()\n",
        "plt.plot(train_accuracies, label='Train Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.title(\"Accuracy Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"accuracy_curve.png\")\n",
        "\n",
        "print(\"Plots saved: loss_curve.png and accuracy_curve.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnF3CN1zohxI"
      },
      "source": [
        "Averages:\n",
        "\n",
        "\n",
        "*   Train Loss: 1.7223, Accuracy: 38.21%, Validation Loss: 1.7194, Accuracy: 38.25%\n",
        "*   Train Loss: 1.7106, Accuracy: 34.80%, Validation Loss: 1.7984, Accuracy: 35.98%\n",
        "\n",
        "*   Train Loss: 1.8150, Accuracy: 34.54%, Val   Loss: 1.7848 Accuracy: 36.13%\n",
        "\n",
        "*   Train Loss: 1.9579, Accuracy: 28.84%, Val   Loss: 1.8691, Accuracy: 32.51%\n",
        "*   Train Loss: 1.9712, Accuracy: 27.54%, Val   Loss: 1.9107 Accuracy: 30.24%\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-YE9-IdEj0-"
      },
      "source": [
        "# Test training data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
